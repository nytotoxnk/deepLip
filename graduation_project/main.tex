\documentclass[12pt]{article}

% ----------------------------------------------------------------------
% Define external packages, language, margins, fonts, new commands 
% and colors
% ----------------------------------------------------------------------
\usepackage[utf8]{inputenc} % Codification
\usepackage[english]{babel} % Writing idiom
\usepackage{multirow}
\usepackage[export]{adjustbox} % Align images
\usepackage{amsmath} % Extra commands for math mode
\usepackage{amssymb} % Mathematical symbols
\usepackage{anysize} % Personalize margins
    \marginsize{2cm}{2cm}{2cm}{2cm} % {left}{right}{above}{below}
\usepackage{appendix} % Appendices
\usepackage{cancel} % Expression cancellation
\usepackage{caption} % Captions
    \captionsetup{labelfont={bf}}
\usepackage{cite} % Citations, like [1 - 3]
\usepackage{color} % Text coloring
\usepackage{fancyvrb}
\usepackage{fancyhdr} % Head note and footnote
    \pagestyle{fancy}
    \fancyhf{}
    %\fancyhead[L]{\footnotesize Graduating } % Left of Head note
    \fancyhead[R]{\footnotesize UNYT} % Right of Head note
    \fancyfoot[L]{\footnotesize Graduating Project} % Left of Footnote
    \fancyfoot[C]{\thepage} % Center of Footnote
    \fancyfoot[R]{\footnotesize Department of Computer Science} % Right of Footnote
    \renewcommand{\footrulewidth}{0.4pt} % Footnote rule
\usepackage{float} % Utilization of [H] in figures
\usepackage{graphicx} % Figures in LaTeX
\usepackage[colorlinks = true, plainpages = true, linkcolor = istblue, urlcolor = istblue, citecolor = istblue, anchorcolor = istblue]{hyperref}
\usepackage{indentfirst} % First paragraph
\usepackage[super]{nth} % Superscripts
\usepackage{siunitx} % SI units
\usepackage{subcaption} % Subfigures
\usepackage{titlesec} % Font
    \titleformat{\section}{\Large\bfseries}{\thesection}{1em}{}
    \titleformat{\subsection}{\large\bfseries}{\thesubsection}{1em}{}
    \titleformat{\subsubsection}{\normalsize\bfseries}{\thesubsubsection}{1em}{}
    \fancyfoot[C]{\thepage}
% Random text (not needed)
\usepackage{duckuments}

% New and re-newcommands
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Specific rule definition
\renewcommand{\appendixpagename}{\LARGE Appendices}
\newcommand{\sectiononlytoc}[1]{%
    \par
    \refstepcounter{section}%
    % Don't print sectional unit; just add to ToC and update the Left header
    \addcontentsline{toc}{section}{\protect\numberline{\thesection}#1}%
    %\fancyhead[L]{#1}% Add chapter title in Left header
}

% Colors
\definecolor{istblue}{RGB}{3, 171, 230}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                 Document                             %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

% ----------------------------------------------------------------------
% Cover
% ----------------------------------------------------------------------
\begin{center}
    \mbox{}\\[2.0cm]
    \LARGE Enabling Multilingual Communication: Automated Lip-synchronization Dubbing for Albanian Videos
\end{center}

\vspace{10cm}

\begin{center}
    \Large ALDO DIKU
\end{center}

\vspace{5cm}

\begin{center}
    \Large UNIVERSITY OF NEW YORK TIRANA
\end{center}
\begin{center}
    \large \Large JULY 2025
\end{center}
\thispagestyle{empty}

\setcounter{page}{0}

\newpage
% ----------------------------------------------------------------------
% APPROVAL PAGE
% ----------------------------------------------------------------------
\noindent This is to certify that I have read this project and that, in my opinion, it is fully adequate, in scope and quality, as a thesis for the degree of Bachelor of Arts in Computer Science.

\vspace{4cm}

\noindent
(Title and Name) \hspace{4cm} \\
(Project Advisor) \hspace{2cm} \underline{\hspace{2cm}} \hspace{1cm} \underline{\hspace{8cm}}

\vspace{3cm}

\noindent This is to confirm that this thesis complies with all the standards set by the Department of Computer Science of University of New York Tirana.

\vspace{2cm}

\noindent Date: \hspace{5cm} \hfill Seal/Signature: \hspace{5cm}

\setcounter{page}{1}

\newpage

% ----------------------------------------------------------------------
% PLAGARIZM CLEARANCE PAGE
% ----------------------------------------------------------------------
\noindent I hereby declare that all information in this document has been obtained and presented in accordance with academic rules and ethical conduct. I also declare that, as required by these rules and conduct, I have fully cited and referenced all material and results that are not original to this work.

\vspace{2cm}

\noindent \hspace{7.5cm} First Name, Last Name:\\
\\
\vspace{1.0cm}
\noindent \hspace{7.3cm} \text{Signature:} % NO CLUE WHY THIS IS 7.3 AND NOT 7.5 LIKE ABOVE

\setcounter{page}{2}

\newpage

% ----------------------------------------------------------------------
% ABSTRACT
% ----------------------------------------------------------------------
\sectiononlytoc{Abstract}
\begin{center}
    
    \vspace{0.5cm}
    
    ABSTRACT
    
    \vspace{1cm}
    
    \centering{ ENABLING MULTILINGUAL COMMUNICATION: AUTOMATED LIP-SYNCHRONIZATION DUBBING FOR ALBANIAN VIDEOS}
    
    \vspace{1cm}
    
    Diku, Aldo.
    
    BA. in Computer Science
    
    Thesis Advisor: Prof. Miralda Çuka
    
    July 2025, 183 pages
    
    \vspace{1cm}
\end{center}

\noindent Using available methods and tools (which i will mention when i gather all the tools and methods) to create lip-synced dubbed videos from an albanian video input and outputting a video of the same speaker in another language with the lips moving according to that languages movements. 
    
\vspace{1cm}
    
\noindent Keywords: Lip-sync, dubbing, albanian language

\newpage

% ----------------------------------------------------------------------
% Acknowledgements
% ----------------------------------------------------------------------

\sectiononlytoc{Acknowledgements}

First and foremost, I would like to express my sincere gratitude to my advisor, Professor Miralda Çuka, for their invaluable guidance, unwavering support, and insightful feedback throughout this project. Their expertise and encouragement were instrumental in shaping this work.

Finally, I am grateful for the support of my family and friends. Something something a little bit longer and better made.

\newpage


% ----------------------------------------------------------------------
% Contents
% ----------------------------------------------------------------------
\tableofcontents 

\newpage

% ----------------------------------------------------------------------
% Body
% ----------------------------------------------------------------------
\section{Introduction}

\subsection{Background of the study}

Option 1\\
The increasing globalisation of online content consumption presents a significant challange for video creators aiming to reach diverse audiences, thereby creating the need to produce multilingual versions of their work. This often involves an increased workload, time and other resources, a perfect example of this is a German content creator in YouTube called der8auer (link?). Der8auer makes the same content in two languages, in english and in german. This workflow includes a lot steps, filming, editing, storing and uploading the content are all things that take time. This project addresses the challange of automatic translation and lip synchronization of video content into multiple languages. 
The main focus of the project will be the Albanian language as an underrepresented and low resource language. To bridge a gap in current research and practical applications of automated video translation and lip synchronization.\\
Option 2\\
Being able to make only one video and have it in multiple languages will help creators but also the target audience as well. Cutting down on the time and money it takes to make two or more videos with the same subject but in different languages. One such case is a youtube creator named der8auer from Germany, for the same topic he creates two videos, one in english and one in german. Sometimes his videos are quite long and having to do them again in another language will tire you out. Having a model that can take your video, translate it into another language and synchronize your lips to the movement of the language of your choosing would be a tool in your arsenal.\\
This project focuses on lip synchronization video dubbing on Albanian language as a low resource and underrepresented language, addressing a significant gap in current research and practical applications.\\
Some of the usages of this lip-sync dubbing technology are: video dubbing and translation, real-time Face-to-Face translation(this would be into the future) and multilingual communication, gaming and virtual environments (some games come in different languages but the characters are coded to move their lips only according to one language, although some companies have done work to change this depending on the language the users select, cyberpunk 2077 used JALI ai for animation lip-sync) reduced cost and labor and time in this case, entertainment and content creation, speech recognition and lip-reading. Film, entertainment and media production. Education and training videos, especially in cases where there is a diverse workforce.\\


\subsection{How lip synchronization works}
Phonemes are the distinct units of sound in speech, or the individual sounds that make up speech and visemes are visual representations of phonemes. Visemes serve to map sounds (phonemes) to corresponding mouth positions or shapes. The goal is to go from the audible (phonemes) to the visual (visemes). Visemes group similar-looking mouth shapes, which reduces the complexity required for animation. Mapping phonemes down to a smaller number of visemes gives artists fewer expressions to pose.\\
The history of lip-syncing comes from animation and in particular two-dimentional cartoons where artists had to make facial movement according to the sounds of speech [\cite{game-lipsynch}].

\subsection{Methodology}
A key limmitation encountered was the lack of suitable Albanian video datasets, specifically open-license content with direct-to-camera speech, reflecting the broader challange of working with low-resource languages in multimedia applications. To address the first issue, custom video contennt was created featuring self-recorded footage in a one by one aspect ratio and lowest camera setting to save .
The first step for this project was the data creation, seeing as there is not a lot of free and publicly available free-use content I can use we had to create the data myself.\\
Videos were recorded at 1440x1440 resolution (1:1 aspect ratio) at 30 frames per second, with durations ranging from 10 seconds up till 5 minutes. The study at (citation needed for the optimal length of time of the videos) found that the optimal length time was for videos with one word in them. To adapt to this, a script was made to create small chunks of videos from a lengthy video to be closer to that optimal format. Videos could have been created by following and reading a script thereby avoiding the transcipiton step, however we want to create a workflow that will take any video of an albanian speaker and turn it into a dub lip synched video.\\
From the videos the next step is audio extraction which will be used for transcription and for the base of the voice cloning tool before inputing it into the final lip synchronization model. For the audio extraction ffmpeg library was used, and audio was extracted in a lossless codec, 16000hz sample rate and signed 16-bit Pulse Code Modulation (PCM), per Google Speech-to-text API documentation \cite{google_api_docs}.
\\
The audio gets sent through the transcription service, and gets saved in a text file along with the name of the transcripted audio file.
When all audio files have been transcripted, they are translated using (PUT TOOL USED HERE) in (PUT NUMBER OF LANGUAGES USED HERE) languages.
(SHOULD MENTION BEFORE WHAT I TRIED FLORJAN ASR FOR EXAMPLE, PESHPERIMA V2, TRAINING OUR OWN MODEL WITH DATASET FROM FLORJAN ETC.) (MAYBE NOT IN THIS SECTION OF THE PAGE????)
(Speech-to-text API from Google Cloud services offered a wide range of languages and Albanian was a supported language, utilizing their Chirp 2 model, the transcriptions were very accurate, even more so as google gives you a confidence score about the sentence or even individual words.)\\
After translating, the translated text and the original audio are used to make the voice clone. We need the original audio of the transcript because of emotions displayed in the audio can be used by the voice cloning tool.\\
Cloned voice and the original video from where the cloned voice was based on are then used to train the lip synchronization model. 
\\Several models (citations needed) were available to try for albanian transcription although with different accuracy. Automatic Speech Recognition (ASR) models are measured using Word Error Rate (WER) where a low score of means perfect translation and a score of 1.0 is the worst case, working on the albanian language I focused only on those models that had support for it. Whisper by OpenAI was one such model, availabe on HuggingFace. To make such model have a better performance (need some results of the model transcribing an audio file) we need to fine-tune it. (THIS SECTION CAN MOVE TO THE "WHAT I TRIED TO DO AND FAILED" SECTION)\\
Here lies another issue with working with a low resource language, not only are there low resources for video, but also for audio which we could use to fine-tune the model. There were two models made especially with albanian language in mind, albanian-asr and peshperima-v2. Because of the low ammount of resources albanian-asr was only able to get to a 46.3\% accuracy, which does not help at all. Peshperima-large-v2 was also not successful when testing. Whisper had a mulltiple sized models which required fine-tuning before being able to use it for transcription. The only trainable model size for a reasonable ammount of time was the whisper-small. The low ammount of data for the audio made the whisper-small not do much better than albanian-asr. \\
Another option was Wav2Vec and its different variations made from Facebook/Meta, this one also failed like the previous models because of lacking datasets. No model tested was able to perform in a state where they could have been used in a real world application, for that reason we needed to switch to a finished model that did not need fine-tuning.\\
Google cloud service offered speech-to-text API options and for many different languages. One of their models was offered for albanian language and had great performance, giving a confidence score about the transcription which would prove to be very usefull since you can use it as a metric to decide if you want to accept the transcription or not. \\
The next step is the preparation of the dataset collected, which was in 1-5 minute video format. From this we extracted audio from the video and tried finding moments of silence inbetween the speaking in order to cut the video into chunks to then later on feed it into the Convolutional Neural Network (CNN).
One of the reasons of why this is a hard problem is that you cannot just get the unique sounds and join them together to form a word, each unique sound changes depending on what is the letter or sound before it. Here is where the deep learning/machine learning helps as it studites large amounts of video and audio to notice these features.
\textbf{One of the challanges of creating a lip synchronization in videos is the need for the lip movements to accurately align with a specified target speech segment, especially in multilingual and unconstrained environment. Visual data falling out of sunc with updated audio and inaccurate lip movements in target videos.}\\
CNN and Generative Adversial Networks (GANs) to create the lip movements that sync up, this combined with a Discriminator Network which would try and detect the GANs fake lip movements and real ones, pushing each other to get better.

\subsection{Work Steps Overview}

From the video we need to get the transcription in albanian, for this step several models were tried and tested although only the google cloud API were the best performing one. The next and easiest step to implement is translating the transcription to the target language, translation has come a long way and there are many tools which can achieve high accuracy, we decided to use googe translate. The following task is to take the translated transcript and use a voice cloning tool/model to make the voice dubbing. \textbf{There are several models availabe which have not been tested by me yet.} After this we have to use object tracking to track the mouth area in the video to then use the deep learning model for lip frame creation. \textbf{This depends heavily on the model the will be applied.}

\section{Transcription}

\subsection{Tests}
From early review there was very little research done on the subject of Automatic Speech Recognition (ASR) for Albanian language, (Florjan citation not sure if it should be applied) Albanian-ASR project in github was a model trained with 40 hours of high quality audio data, using DeepSpeech architecture and additional mechanisms such as Attention mechanism and context-dependent phoneme modeling to enhance the accuracy of the Albanian language ASR system. The low ammount of hours led the model to have a low accuracy of 46.3\% which cannot be used in real-world applications.\\
Whisper made by OpenAI had support for Albanian language but its transcription capabilities were also not able to perform much better on Albanian language \cite{openai_whisper}. Trying to fine tune the model with the 40 hours of albanian audio provided by the Albanian-ASR proved challanging when trying to fine-tune their largest whisper-large-v3 model (1.55 Billion parameters). With the current hardware being an Nvidia 3080ti 12GB graphics card, it takes 177 hours for the largest model to train. Only when we moved on to the tiny model (39 Million parameters)was the training time less than 24 hours. 
\subsection{Audio}
The first step of the project is extracting audio from the videos, this was done using ffmpeg. Going by the suggestions of googles speech-to-text API best practices, the audio was sampled at 16000 Hz, converted into a singned-integer bitrate, and in a lossless format.\\
A simple test was done to check which specs were best suited for the videos we were creating for the dataset. These were all tested using googles speech-to-text API giving a confidence score for each of the tests. If we are going to do tests.\\  \textbf{Need to add the test results here. \\ Might need to split audio file into two mono files or downmix into one mono file.}
\\
One group of audio files were made in a lossless format, signed 16bit PCM (linear16), 16000hz sampling rate. While the other folder, was made with the same codec but with 48000hz sampling rate.  
\\
\subsection{Notes}
The google api workflow was difficult to implement for it to work correctly and there were more lines of code to implement it than the other models. The accuracy of the API was not the best, but it was the only one that gave a confidence score for each of the tests. A new model made by Kushtrim Vioska, based on openai's whisper-large-v3-turbo and trained on 200 hours of albanian audio was able to transcribe the audio with a great accuracy, unfortuantely the model at this time does not have the weights open source and was only able to be used through a gradio app which slowed down the process of transcribing the audio significantly and the resources of the huggingface model are volitale because of the amount of requests it gets.\\
Going through the next step of the workflow we move from the transciption to the translation of the text, which can be done with high accuracy using a lot of different models which \textbf{need to be tested and researched more thoroughly}. \\
Using the translated text into the language of our choice we can then try and use a voice cloning model to make the voice dubbing. A lot of models were available for this task, Nari Labs released Dia, in 22nd April 2025, coqui-ai, which a lot of other models are based on. 
A paper in 5th of May 2025 was released by \textbf{Yemin Shi et al. Voila: Voice-Language Foundation Models for Real-Time Autonomous Interaction and Voice Role-Play} which created a foundation model for realt-time autonomous interaction and voice role-play. https://arxiv.org/abs/2505.02707. \\
\textbf{It can be posisble to make a comparison system between the google API and the Kushtrim Vioska model to see how much do they match. Not sure what we can achieve with this. Google API does give us a confidence score for each of the tests. We can probably use this to see how much the model is confident in the transcription and then use that to our advantage. If we also use the word confidence we can then compare to see if Kushtrims model made a different transcription, whenever the confidence is low we can use Kushtrims model to make a transcription.}\\
Additionaly, \cite{PAWAR2024100084} used "vVISWA" dataset which hade isolated words or independent speech, which worked great in combining the audio and video in the trainning set, and according to them reducing overfitting due to its inherent data augmentation effect.
\newpage
\subsection{Studying the literature}
There are a lot of models and papers which have been made for the task of lip-synchronization, but most of them are focused on english and other high resource languages. Pawar, D. et al. (2024) used Generative Adversarial Networks (GANs), using both audio and visual features techiques like MFCCs and VGG-M-based CNNs to achieve accurate lip-synchronization. \cite{PAWAR2024100084} shows some of the models using GANs as their architecture. \cite{Li_2021} used a combination of a deep neural network with one dimension and a Long Short-Term Memory (LSTM) to generate a face model from speech input.\\
Traditionally, methods required highly controlled and precisely aligned datasets, making them less flexible and scalable for diverse inputs. Newer approaches aim to reduce this dependency. 
\subsection{Questions raised}
List of questions:
\begin{itemize}
    \item How do we decide if we are going to use a transcription or not?
    \item How do we know if the transcription is correct programatically?
    \item How should the data be prepared for the lip-synchronization model?
    \item Video format, 30 frames per second or 25, quality of the video, resolution, etc.
    \item Audio format, mp3, lossless, bitrate, etc.
    \item How does the duration of video chunks used during preprocessing impact the performance of the final machine learning model, and what chunk length yields the best trade-off between information retention and computational efficiency?
    \item How are voice cloning models evaluated, as it feels like it should be done with a human evaluation and that is faulty most of the time?
    \item Which would be the best model for the lip-synchronization task based on the data we have?
    \item Best way to handle differences between original audio timing and the voice cloned sample timing. (slow down the voice? speedup)
\end{itemize}
AS for the optimal length of the video chunks, the study done by \cite{PAWAR2024100084} faced challanges with lengthy continuous speech, inlcuding time-consuming merging issues and potential overfitting. This was addressed by using the "vVISWA" dataset which contains isolated words or indepentent speech. This improved the performance of the model a lot and reduced overfitting.


List of challanges:
\begin{itemize}
    \item Low resources for the datasets in albanian, in video and audio.
    \item No open source models for the transcription of the audio from albanian.
    \item Voice cloning models being evaluated using a human evaluation and that is biased.
    \item Storage of data, both in the cloud and locally.
    \item Computing power.
    \item Training time.
    \item If the spoken albanian is with a heavy dialect, an Albanian ASR trained with large number dialects is needed
    \item If the cutting of videos is not done correctly, it is possible to cut a word in half
    \item Translating the transcribed albanian dialect. How possible it is? NLP script transformation + translate*
    \item Voice cloning emotions.
    \item Video chunking first then transcription, or transcription (have to keep timings) then chunking - might be more possible and more cost effective than the NLP to find the problems in the language/chunked words but not for dialects.
    \item Could probably use the timings from google speech-to-text api to make the cuts since it offers that. Making the better option to be transcript first then cut/chunk second.  
    \item Multiple Ethical challanges
    \item Are NLP-s better than LLM-s in these cases/uses.?
\end{itemize}

List of improvements:
\begin{itemize}
    \item Better dataset, quantity and quality, diversity (age, gender, moustache, skin color, lip color- lipstick, ethnicity, also in this case, dialect)
    \item Separation of dataset into single word video for better model performance.
    \item Albanian ASR open-source model, google speech-to-text API is paid service, and Kushtrims uses huggingface spaces which is slow
    \item Comparison between Google Speech-to-text API and Kushtrim ASR
    \item Addition on an NLP to handle transcription errors
    \item NLP for dialect speech, to transform it into something that can be translated, maybe LLM is better in this case.
    \item Better voice cloning. More emotional expression
    \item Better face reproduction from the emotions of the voice clone.
    \item Real time lip-synch dubbing
    \item Separating the dataset into two groups the new dataset with videos close to one word per video and see the results of that and the original dataset with longer videos. 
    \item NLP can also be utilized in the audio length check, if we have too much of a difference in audio length, we could apply NLP-s to change the sentence into something shorter or longer but keeping the sentence about the same.
\end{itemize}

\bibliography{references}
\bibliographystyle{plain}

\end{document}