@article{PAWAR2024100084,
title = {Generating dynamic lip-syncing using target audio in a multimedia environment},
journal = {Natural Language Processing Journal},
volume = {8},
pages = {100084},
year = {2024},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2024.100084},
url = {https://www.sciencedirect.com/science/article/pii/S2949719124000323},
author = {Diksha Pawar and Prashant Borde and Pravin Yannawar},
keywords = {AVSR, GAN, vVISWa dataset, LipChanger, LSTM},
abstract = {The presented research focuses on the challenging task of creating lip-sync facial videos that align with a specified target speech segment. A novel deep-learning model has been developed to produce precise synthetic lip movements corresponding to the speech extracted from an audio source. Consequently, there are instances where portions of the visual data may fall out of sync with the updated audio and this challenge is handled through, a novel strategy, leveraging insights from a robust lip-sync discriminator. Additionally, this study introduces fresh criteria and evaluation benchmarks for assessing lip synchronization in unconstrained videos. LipChanger demonstrates improved PSNR values, indicative of enhanced image quality. Furthermore, it exhibits highly accurate lip synthesis, as evidenced by lower LMD values and higher SSIM values. These outcomes suggest that the LipChanger approach holds significant potential for enhancing lip synchronization in talking face videos, resulting in more realistic lip movements. The proposed LipChanger model and its associated evaluation benchmarks show promise and could potentially contribute to advancements in lip-sync technology for unconstrained talking face videos.}
}
@inproceedings{Li_2021,
   title={A Novel Speech-Driven Lip-Sync Model with CNN and LSTM},
   url={http://dx.doi.org/10.1109/CISP-BMEI53629.2021.9624360},
   DOI={10.1109/cisp-bmei53629.2021.9624360},
   booktitle={2021 14th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)},
   publisher={IEEE},
   author={Li, Xiaohong and Wang, Xiang and Wang, Kai and Lian, Shiguo},
   year={2021},
   month=oct, pages={1–6}
}
@manual{google_api_docs,
  author = {Google Cloud},
  title = {Speech to Text API},
  url = {https://cloud.google.com/speech-to-text/v2/docs},
}
@techreport{game-lipsynch,
  author = {Jonathan Gratch, Arien Kock},
  institution = {University of Twente, Department of Computer Science, University of SOurthern California, Institute for Creative Technologies},
  title = {An Evaluation of Automatic Lip-syncing Methods for Game Environments},
  year = {2022}
}
@techreport{openai_whisper,
  author = {OpenAI},
  institution = {OpenAI},
  title = {Robust Speech Recognition via Large-Scale Weak Supervision},
  year = {2022}
}
@misc{NAUTILUS,
      title={NAUTILUS: a Versatile Voice Cloning System}, 
      author={Hieu-Thi Luong and Junichi Yamagishi},
      year={2020},
      eprint={2005.11004},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url={https://arxiv.org/abs/2005.11004}, 
}
@inproceedings{Unified_system_for_Voice_Cloning_and_Voice_Conversion,
  title     = {A Unified System for Voice Cloning and Voice Conversion through Diffusion Probabilistic Modeling},
  author    = {Tasnima Sadekova and Vladimir Gogoryan and Ivan Vovk and Vadim Popov and Mikhail Kudinov and Jiansheng Wei},
  year      = {2022},
  booktitle = {Interspeech 2022},
  pages     = {3003--3007},
  doi       = {10.21437/Interspeech.2022-10879},
  issn      = {2958-1796},
}
@article{Real-time_voice_cloning_system,
    doi = {10.1371/journal.pone.0283440},
    author = {Hu, Weixin AND Zhu, Xianyou},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {A real-time voice cloning system with multiple algorithms for speech quality improvement},
    year = {2023},
    month = {04},
    volume = {18},
    url = {https://doi.org/10.1371/journal.pone.0283440},
    pages = {1-14},
    abstract = {With the development of computer technology, speech synthesis techniques are becoming increasingly sophisticated. Speech cloning can be performed as a subtask of speech synthesis technology by using deep learning techniques to extract acoustic information from human voices and combine it with text to output a natural human voice. However, traditional speech cloning technology still has certain limitations; excessively large text inputs cannot be adequately processed, and the synthesized audio may include noise artifacts like breaks and unclear phrases. In this study, we add a text determination module to a synthesizer module to process words the model has not included. The original model uses fuzzy pronunciation for such words, which is not only meaningless but also affects the entire sentence. Thus, we improve the model by splitting the letters and pronouncing them separately. Finally, we also improved the preprocessing and waveform conversion modules of the synthesizer. We replace the pre-net module of the synthesizer and use an upgraded noise reduction algorithm combined with the SV2TTS framework to achieve a system with superior speech synthesis performance. Here, we focus on improving the performance of the synthesizer module to achieve higher-quality speech synthesis audio output.},
    number = {4},

}
@misc{viola-voice-role-play,
      title={Voila: Voice-Language Foundation Models for Real-Time Autonomous Interaction and Voice Role-Play}, 
      author={Yemin Shi and Yu Shu and Siwei Dong and Guangyi Liu and Jaward Sesay and Jingwen Li and Zhiting Hu},
      year={2025},
      eprint={2505.02707},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2505.02707}, 
}
@article{ye2023geneface++,
  title={GeneFace++: Generalized and Stable Real-Time Audio-Driven 3D Talking Face Generation},
  author={Ye, Zhenhui and He, Jinzheng and Jiang, Ziyue and Huang, Rongjie and Huang, Jiawei and Liu, Jinglin and Ren, Yi and Yin, Xiang and Ma, Zejun and Zhao, Zhou},
  journal={arXiv preprint arXiv:2305.00787},
  year={2023}
}
@article{openvoice,
  title={OpenVoice: Versatile Instant Voice Cloning},
  author={Qin, Zengyi and Zhao, Wenliang and Yu, Xumin and Sun, Xin},
  journal={arXiv preprint arXiv:2312.01479},
  year={2023}
}
@misc{GoogleFaceDetector,
  title = {{MediaPipe Face Detector | Google for Developers}},
  author = {{Google}}, 
  howpublished = {\url{https://ai.google.dev/edge/mediapipe/solutions/vision/face_detector}},
  year = {2024},
  note = {Accessed: 2025-06-11}
}
@Article{biomedinformatics4010023,
  author = {Exarchos, Themis and Dimitrakopoulos, Georgios N. and Vrahatis, Aristidis G. and Chrysovitsiotis, Georgios and Zachou, Zoi and Kyrodimos, Efthymios},
  title = {Lip-Reading Advancements: A 3D Convolutional Neural Network/Long Short-Term Memory Fusion for Precise Word Recognition},
  journal = {BioMedInformatics},
  volume = {4},
  year = {2024},
  number = {1},
  pages = {410--422},
  url = {https://www.mdpi.com/2673-7426/4/1/23},
  issn = {2673-7426},
  abstract = {Lip reading, the art of deciphering spoken words from the visual cues of lip movements, has garnered significant interest for its potential applications in diverse fields, including assistive technologies, human–computer interaction, and security systems. With the rapid advancements in technology and the increasing emphasis on non-verbal communication methods, the significance of lip reading has expanded beyond its traditional boundaries. These technological advancements have led to the generation of large-scale and complex datasets, necessitating the use of cutting-edge deep learning tools that are adept at handling such intricacies. In this study, we propose an innovative approach combining 3D Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) networks to tackle the challenging task of word recognition from lip movements. Our research leverages a meticulously curated dataset, named MobLip, encompassing various speech patterns, speakers, and environmental conditions. The synergy between the spatial information extracted by 3D CNNs and the temporal dynamics captured by LSTMs yields impressive results, achieving an accuracy rate of up to 87.5%, showcasing robustness to lighting variations and speaker diversity. Comparative experiments demonstrate our model’s superiority over existing lip-reading approaches, underlining its potential for real-world deployment. Furthermore, we discuss ethical considerations and propose avenues for future research, such as multimodal integration with audio data and expanded language support. In conclusion, our 3D CNN-LSTM architecture presents a promising solution to the complex problem of word recognition from lip movements, contributing to the advancement of communication technology and opening doors to innovative applications in an increasingly visual world.},
  DOI = {10.3390/biomedinformatics4010023}
}
@INPROCEEDINGS{earliest-lip-synch,
  author={Koster, B.E. and Rodman, R.D. and Bitzer, D.},
  booktitle={Proceedings of 1994 28th Asilomar Conference on Signals, Systems and Computers}, 
  title={Automated lip-sync: direct translation of speech-sound to mouth-shape}, 
  year={1994},
  volume={1},
  number={},
  pages={583-586 vol.1},
  keywords={Mouth;Shape;Tongue;Teeth;Strontium;Natural languages;Loudspeakers;Speech recognition;Facial animation;Lips},
  doi={10.1109/ACSSC.1994.471519}
}
@article{ObamaNet,
  author = {Suwajanakorn, Supasorn and Seitz, Steven M. and Kemelmacher-Shlizerman, Ira},
  title = {Synthesizing Obama: learning lip sync from audio},
  year = {2017},
  issue_date = {August 2017},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {36},
  number = {4},
  issn = {0730-0301},
  url = {https://doi.org/10.1145/3072959.3073640},
  doi = {10.1145/3072959.3073640},
  abstract = {Given audio of President Barack Obama, we synthesize a high quality video of him speaking with accurate lip sync, composited into a target video clip. Trained on many hours of his weekly address footage, a recurrent neural network learns the mapping from raw audio features to mouth shapes. Given the mouth shape at each time instant, we synthesize high quality mouth texture, and composite it with proper 3D pose matching to change what he appears to be saying in a target video to match the input audio track. Our approach produces photorealistic results.},
  journal = {ACM Trans. Graph.},
  month = jul,
  articleno = {95},
  numpages = {13},
  keywords = {videos, uncanny valley, lip sync, face synthesis, big data, audiovisual speech, audio, RNN, LSTM}
}
@Article{Audio-driven-facial-animation,
  AUTHOR = {Jiang, Diqiong and Chang, Jian and You, Lihua and Bian, Shaojun and Kosk, Robert and Maguire, Greg},
  TITLE = {Audio-Driven Facial Animation with Deep Learning: A Survey},
  JOURNAL = {Information},
  VOLUME = {15},
  YEAR = {2024},
  NUMBER = {11},
  ARTICLE-NUMBER = {675},
  URL = {https://www.mdpi.com/2078-2489/15/11/675},
  ISSN = {2078-2489},
  ABSTRACT = {Audio-driven facial animation is a rapidly evolving field that aims to generate realistic facial expressions and lip movements synchronized with a given audio input. This survey provides a comprehensive review of deep learning techniques applied to audio-driven facial animation, with a focus on both audio-driven facial image animation and audio-driven facial mesh animation. These approaches employ deep learning to map audio inputs directly onto 3D facial meshes or 2D images, enabling the creation of highly realistic and synchronized animations. This survey also explores evaluation metrics, available datasets, and the challenges that remain, such as disentangling lip synchronization and emotions, generalization across speakers, and dataset limitations. Lastly, we discuss future directions, including multi-modal integration, personalized models, and facial attribute modification in animations, all of which are critical for the continued development and application of this technology.},
  DOI = {10.3390/info15110675}
}
@ARTICLE{speech-driven-expression,
  author={Sadoughi, Najmeh and Busso, Carlos},
  journal={IEEE Transactions on Affective Computing}, 
  title={Speech-Driven Expressive Talking Lips with Conditional Sequential Generative Adversarial Networks}, 
  year={2021},
  volume={12},
  number={4},
  pages={1031-1044},
  keywords={Hidden Markov models;Speech recognition;Adaptation models;Training data;Data models;Lips;Generative adversarial networks;Speech-driven model;lip movements;expressive and naturalistic lip movements;generative adversarial network},
  doi={10.1109/TAFFC.2019.2916031}
}
@misc{neura,
  author = {Neura},
  howpublished = {\url{https://neura.al/}},
  title = {Neura.al},
  year = {2025}
}
@misc{kushtrim,
  author = {{Kushtrim}},
  howpublished = {\url{https://huggingface.co/spaces/Kushtrim/whisper-large-v3-turbo-shqip}},
  title = {Whisper Large V3 Turbo Shqip},
  year = {2025}
}
@misc{florijan-qosja,
  author = {Florijan Qosja},
  howpublished = {\url{https://github.com/florijanqosja/Albanian-ASR}},
  title = {Development of an Albanian Language Transcriber using Artificial Intelligence},
  year = {2023}
}
@misc{deep-speech,
      title={Deep Speech: Scaling up end-to-end speech recognition}, 
      author={Awni Hannun and Carl Case and Jared Casper and Bryan Catanzaro and Greg Diamos and Erich Elsen and Ryan Prenger and Sanjeev Satheesh and Shubho Sengupta and Adam Coates and Andrew Y. Ng},
      year={2014},
      eprint={1412.5567},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1412.5567}, 
}
@misc{facebook-wav2vec,
      title={wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations}, 
      author={Alexei Baevski and Henry Zhou and Abdelrahman Mohamed and Michael Auli},
      year={2020},
      eprint={2006.11477},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@techreport{automatic-lip-synch-methods-in-games,
  author = {Kock, Arien ; Gratch, Jonathan},
  institution = {UNIVERSITY OF SOUTHERN CALIFORNIA LOS ANGELES},
  title = {An Evaluation of Automatic Lip-syncing Methods for Game Environments},
  year = {2005}
}
@article{https://doi.org/10.1002/vis.4340020404,
author = {Lewis, John},
title = {Automated lip-sync: Background and techniques},
journal = {The Journal of Visualization and Computer Animation},
volume = {2},
number = {4},
pages = {118-122},
keywords = {Facial animation, Speech synchronization},
doi = {https://doi.org/10.1002/vis.4340020404},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/vis.4340020404},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/vis.4340020404},
abstract = {Abstract The problem of creating mouth animation synchronized to recorded speech is discussed. Review of a model of speech sound generation indicates that the automatic derivation of mouth movement from a speech soundtrack is a tractable problem. Several automatic lip-sync techniques are compared, and one method is described in detail. In this method a common speech synthesis method, linear prediction, is adapted to provide simple and accurate phoneme recognition. The recognized phonemes are associated with mouth positions to provide keyframes for computer animation of speech. Experience with this technique indicates that automatic lip-sync can produce useful results.},
year = {1991}
}
@Article{computers14010007,
  AUTHOR = {Rafiei Oskooei, Amirkia and Aktaş, Mehmet S. and Keleş, Mustafa},
  TITLE = {Seeing the Sound: Multilingual Lip Sync for Real-Time Face-to-Face Translation},
  JOURNAL = {Computers},
  VOLUME = {14},
  YEAR = {2025},
  NUMBER = {1},
  ARTICLE-NUMBER = {7},
  URL = {https://www.mdpi.com/2073-431X/14/1/7},
  ISSN = {2073-431X},
  ABSTRACT = {Imagine a future where language is no longer a barrier to real-time conversations, enabling instant and lifelike communication across the globe. As cultural boundaries blur, the demand for seamless multilingual communication has become a critical technological challenge. This paper addresses the lack of robust solutions for real-time face-to-face translation, particularly for low-resource languages, by introducing a comprehensive framework that not only translates language but also replicates voice nuances and synchronized facial expressions. Our research tackles the primary challenge of achieving accurate lip synchronization across culturally diverse languages, filling a significant gap in the literature by evaluating the generalizability of lip sync models beyond English. Specifically, we develop a novel evaluation framework combining quantitative lip sync error metrics and qualitative assessments by human observers. This framework is applied to assess two state-of-the-art lip sync models with different architectures for Turkish, Persian, and Arabic languages, using a newly collected dataset. Based on these findings, we propose and implement a modular system that integrates language-agnostic lip sync models with neural networks to deliver a fully functional face-to-face translation experience. Inference Time Analysis shows this system achieves highly realistic, face-translated talking heads in real time, with a throughput as low as 0.381 s. This transformative framework is primed for deployment in immersive environments such as VR/AR, Metaverse ecosystems, and advanced video conferencing platforms. It offers substantial benefits to developers and businesses aiming to build next-generation multilingual communication systems for diverse applications. While this work focuses on three languages, its modular design allows scalability to additional languages. However, further testing in broader linguistic and cultural contexts is required to confirm its universal applicability, paving the way for a more interconnected and inclusive world where language ceases to hinder human connection.},
  DOI = {10.3390/computers14010007}
}