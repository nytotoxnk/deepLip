@article{PAWAR2024100084,
title = {Generating dynamic lip-syncing using target audio in a multimedia environment},
journal = {Natural Language Processing Journal},
volume = {8},
pages = {100084},
year = {2024},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2024.100084},
url = {https://www.sciencedirect.com/science/article/pii/S2949719124000323},
author = {Diksha Pawar and Prashant Borde and Pravin Yannawar},
keywords = {AVSR, GAN, vVISWa dataset, LipChanger, LSTM},
abstract = {The presented research focuses on the challenging task of creating lip-sync facial videos that align with a specified target speech segment. A novel deep-learning model has been developed to produce precise synthetic lip movements corresponding to the speech extracted from an audio source. Consequently, there are instances where portions of the visual data may fall out of sync with the updated audio and this challenge is handled through, a novel strategy, leveraging insights from a robust lip-sync discriminator. Additionally, this study introduces fresh criteria and evaluation benchmarks for assessing lip synchronization in unconstrained videos. LipChanger demonstrates improved PSNR values, indicative of enhanced image quality. Furthermore, it exhibits highly accurate lip synthesis, as evidenced by lower LMD values and higher SSIM values. These outcomes suggest that the LipChanger approach holds significant potential for enhancing lip synchronization in talking face videos, resulting in more realistic lip movements. The proposed LipChanger model and its associated evaluation benchmarks show promise and could potentially contribute to advancements in lip-sync technology for unconstrained talking face videos.}
}
@inproceedings{Li_2021,
   title={A Novel Speech-Driven Lip-Sync Model with CNN and LSTM},
   url={http://dx.doi.org/10.1109/CISP-BMEI53629.2021.9624360},
   DOI={10.1109/cisp-bmei53629.2021.9624360},
   booktitle={2021 14th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)},
   publisher={IEEE},
   author={Li, Xiaohong and Wang, Xiang and Wang, Kai and Lian, Shiguo},
   year={2021},
   month=oct, pages={1â€“6} }
@manual{google_api_docs,
  author = {Google Cloud},
  title = {Speech to Text API},
  url = {https://cloud.google.com/speech-to-text/v2/docs},
}
@techreport{game-lipsynch,
  author = {Jonathan Gratch, Arien Kock},
  institution = {University of Twente, Department of Computer Science, University of SOurthern California, Institute for Creative Technologies},
  title = {An Evaluation of Automatic Lip-syncing Methods for Game Environments},
  year = {2022}
}
@techreport{openai_whisper,
  author = {OpenAI},
  institution = {OpenAI},
  title = {Robust Speech Recognition via Large-Scale Weak Supervision},
  year = {2022}
}
@misc{NAUTILUS,
      title={NAUTILUS: a Versatile Voice Cloning System}, 
      author={Hieu-Thi Luong and Junichi Yamagishi},
      year={2020},
      eprint={2005.11004},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url={https://arxiv.org/abs/2005.11004}, 
}
@inproceedings{Unified_system_for_Voice_Cloning_and_Voice_Conversion,
  title     = {A Unified System for Voice Cloning and Voice Conversion through Diffusion Probabilistic Modeling},
  author    = {Tasnima Sadekova and Vladimir Gogoryan and Ivan Vovk and Vadim Popov and Mikhail Kudinov and Jiansheng Wei},
  year      = {2022},
  booktitle = {Interspeech 2022},
  pages     = {3003--3007},
  doi       = {10.21437/Interspeech.2022-10879},
  issn      = {2958-1796},
}
@article{Real-time_voice_cloning_system,
    doi = {10.1371/journal.pone.0283440},
    author = {Hu, Weixin AND Zhu, Xianyou},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {A real-time voice cloning system with multiple algorithms for speech quality improvement},
    year = {2023},
    month = {04},
    volume = {18},
    url = {https://doi.org/10.1371/journal.pone.0283440},
    pages = {1-14},
    abstract = {With the development of computer technology, speech synthesis techniques are becoming increasingly sophisticated. Speech cloning can be performed as a subtask of speech synthesis technology by using deep learning techniques to extract acoustic information from human voices and combine it with text to output a natural human voice. However, traditional speech cloning technology still has certain limitations; excessively large text inputs cannot be adequately processed, and the synthesized audio may include noise artifacts like breaks and unclear phrases. In this study, we add a text determination module to a synthesizer module to process words the model has not included. The original model uses fuzzy pronunciation for such words, which is not only meaningless but also affects the entire sentence. Thus, we improve the model by splitting the letters and pronouncing them separately. Finally, we also improved the preprocessing and waveform conversion modules of the synthesizer. We replace the pre-net module of the synthesizer and use an upgraded noise reduction algorithm combined with the SV2TTS framework to achieve a system with superior speech synthesis performance. Here, we focus on improving the performance of the synthesizer module to achieve higher-quality speech synthesis audio output.},
    number = {4},

}
@misc{viola-voice-role-play,
      title={Voila: Voice-Language Foundation Models for Real-Time Autonomous Interaction and Voice Role-Play}, 
      author={Yemin Shi and Yu Shu and Siwei Dong and Guangyi Liu and Jaward Sesay and Jingwen Li and Zhiting Hu},
      year={2025},
      eprint={2505.02707},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2505.02707}, 
}