@article{PAWAR2024100084,
title = {Generating dynamic lip-syncing using target audio in a multimedia environment},
journal = {Natural Language Processing Journal},
volume = {8},
pages = {100084},
year = {2024},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2024.100084},
url = {https://www.sciencedirect.com/science/article/pii/S2949719124000323},
author = {Diksha Pawar and Prashant Borde and Pravin Yannawar},
keywords = {AVSR, GAN, vVISWa dataset, LipChanger, LSTM},
abstract = {The presented research focuses on the challenging task of creating lip-sync facial videos that align with a specified target speech segment. A novel deep-learning model has been developed to produce precise synthetic lip movements corresponding to the speech extracted from an audio source. Consequently, there are instances where portions of the visual data may fall out of sync with the updated audio and this challenge is handled through, a novel strategy, leveraging insights from a robust lip-sync discriminator. Additionally, this study introduces fresh criteria and evaluation benchmarks for assessing lip synchronization in unconstrained videos. LipChanger demonstrates improved PSNR values, indicative of enhanced image quality. Furthermore, it exhibits highly accurate lip synthesis, as evidenced by lower LMD values and higher SSIM values. These outcomes suggest that the LipChanger approach holds significant potential for enhancing lip synchronization in talking face videos, resulting in more realistic lip movements. The proposed LipChanger model and its associated evaluation benchmarks show promise and could potentially contribute to advancements in lip-sync technology for unconstrained talking face videos.}
}